{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "636ca382-1d0d-4313-b5ca-b2112250060c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Project 2 - Trading Meme Stocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef46536-c727-4be2-a35f-b85866629e51",
   "metadata": {},
   "source": [
    "## Imports, environment variables and keys "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ff9f48f2-f45b-4766-b481-d91a8e2c90ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "\n",
    "import time\n",
    "import hvplot.pandas\n",
    "from pathlib import Path\n",
    "from datetime import date\n",
    "from datetime import datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "import alpaca_trade_api as tradeapi\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "from pandas.tseries.offsets import Hour\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import joblib\n",
    "\n",
    "\n",
    "\n",
    "# Basic functionalities\n",
    "import json\n",
    "\n",
    "\n",
    "# datetime manipulation\n",
    "import datetime as dt\n",
    "from time import sleep\n",
    "from datetime import timedelta\n",
    "\n",
    "# Deep learning\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Deep learning model persistence\n",
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "# Graphing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px \n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d9c709-b966-43d7-aae4-3565fd096887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "#Set API Key from env variable\n",
    "ss_key=os.getenv(\"SS_API_KEY\")\n",
    "\n",
    "# Set Alpaca API key and secret\n",
    "alpaca_api_key = os.getenv(\"ALPACA_API_KEY\")\n",
    "alpaca_secret_key = os.getenv(\"ALPACA_SECRET_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a09b64-ead8-4a4a-84c4-3193f45fafe2",
   "metadata": {},
   "source": [
    "## Analysis Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0176edfd-e3c9-4be2-a0f5-543333406050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_alpaca_connection():    \n",
    "    api = tradeapi.REST(\n",
    "    alpaca_api_key,\n",
    "    alpaca_secret_key,\n",
    "    base_url = 'https://paper-api.alpaca.markets',\n",
    "    api_version = \"v2\"\n",
    "    )\n",
    "    return api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4643a554-8d2c-41be-ab51-c878a717e99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_df():\n",
    "    # Create headers variable containing API Key \n",
    "    headers = {\n",
    "        'Authorization' : f'Token {ss_key}',\n",
    "        'Accept': 'application/json',\n",
    "    }\n",
    "    # Get data for daily stock sentiment from API\n",
    "    sentiment_response = requests.get('https://socialsentiment.io/api/v1/stocks/sentiment/daily/', headers=headers)\n",
    "    \n",
    "    # Create dict from sentiment_response['results'] and then a data frame from this dict \n",
    "    sentiment_dict = sentiment_response.json()['results']\n",
    "    sentiment_df = pd.DataFrame.from_dict(sentiment_dict)\n",
    "    \n",
    "    # Determine how many lines and then pages are in the sentiment response\n",
    "    line_count = sentiment_response.json()['count']\n",
    "    page_count = int(line_count / 50) + (line_count % 50 > 0)\n",
    "    page=2\n",
    "    \n",
    "    # Loop through each page and gather the sentiment data from the API\n",
    "    while page <= page_count:\n",
    "        # Loop through all pages available from API and construct dataframe for sentiment data\n",
    "        sentiment_url = \"https://socialsentiment.io/api/v1/stocks/sentiment/daily/?page=%s\"%page\n",
    "        sentiment_response = requests.get(sentiment_url, headers=headers)\n",
    "        sentiment_dict = sentiment_response.json()['results']\n",
    "        sentiment_df_loop = pd.json_normalize(sentiment_dict)\n",
    "        sentiment_df = pd.concat([sentiment_df, sentiment_df_loop], axis=0)\n",
    "        page += 1\n",
    "        time.sleep(1)\n",
    "\n",
    "    # Reset the index on the sentiment df and drop the old index\n",
    "    sentiment_df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    # Output the sentiment data for today to a csv file\n",
    "    path = (f'../Resources/sentiment_{date.today()}.csv')\n",
    "    sentiment_df.to_csv(path)\n",
    "    \n",
    "    return sentiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f15b53-e9a2-43e4-9d67-0ba4a190a45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_trending_df():\n",
    "    # Create headers variable containing API Key \n",
    "    headers = {\n",
    "        'Authorization' : f'Token {ss_key}',\n",
    "        'Accept': 'application/json',\n",
    "    }\n",
    "    # Get data for trending stock sentiment from API\n",
    "    trending_response = requests.get('https://socialsentiment.io/api/v1/stocks/trending/daily/', headers=headers)\n",
    "    \n",
    "    # Create dict from trending_response and then a data frame from this dict \n",
    "    trending_dict = trending_response.json()\n",
    "    trending_df = pd.DataFrame.from_dict(trending_dict)\n",
    "    \n",
    "    # Output the sentiment data for today to a csv file\n",
    "    path = (f'../Resources/sentiment_trending_{date.today()}.csv')\n",
    "    trending_df.to_csv(path)\n",
    "    \n",
    "    return trending_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1768a05-d793-4af6-903e-e892a8c2b3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_technical_analysis_df():\n",
    "    # Create the Alpaca API object, specifying use of the paper trading account:\n",
    "    api = create_alpaca_connection()\n",
    "    \n",
    "    # Set the list of tickers to the top class stock\n",
    "    tickers = top_class_stocks.index\n",
    "    # Set get data from API to DataFrame\n",
    "    today = datetime.now()\n",
    "    yesterday = today - timedelta(days=1)\n",
    "    previous_days = today - timedelta(days=365)\n",
    "    beg_date = previous_days\n",
    "    end_date = yesterday\n",
    "    timeframe='1Day'\n",
    "    start =  pd.Timestamp(f'{beg_date} 09:30:00-0400', tz='America/New_York').replace(hour=9, minute=30, second=0).astimezone('GMT').isoformat()[:-6]+'Z'\n",
    "    end =  pd.Timestamp(f'{end_date} 16:00:00-0400', tz='America/New_York').replace(hour=16, minute=0, second=0).astimezone('GMT').isoformat()[:-6]+'Z'\n",
    "    portfolio_df = api.get_bars(tickers, timeframe, start=start, end=end).df\n",
    "   \n",
    "    \n",
    "    # Calculate the 5,15 period high and low rolling SMAs and add each to the portfolio dataframe\n",
    "    portfolio_df['Lowest_5D'] = portfolio_df.groupby('symbol')['low'].transform(lambda x: x.rolling(window = 12).min())\n",
    "    portfolio_df['High_5D'] = portfolio_df.groupby('symbol')['high'].transform(lambda x: x.rolling(window = 12).max())\n",
    "    portfolio_df['Lowest_15D'] = portfolio_df.groupby('symbol')['low'].transform(lambda x: x.rolling(window = 26).min())\n",
    "    portfolio_df['High_15D'] = portfolio_df.groupby('symbol')['high'].transform(lambda x: x.rolling(window = 26).max())\n",
    "    \n",
    "    # Calculate Stochastic Indicators and add each to the portfolio dataframe\n",
    "    portfolio_df['Stochastic_5'] = ((portfolio_df['close'] - portfolio_df['Lowest_5D'])/(portfolio_df['High_5D'] - portfolio_df['Lowest_5D']))*100\n",
    "    portfolio_df['Stochastic_15'] = ((portfolio_df['close'] - portfolio_df['Lowest_15D'])/(portfolio_df['High_15D'] - portfolio_df['Lowest_15D']))*100\n",
    "    portfolio_df['Stochastic_%D_5'] = portfolio_df['Stochastic_5'].rolling(window = 5).mean()\n",
    "    portfolio_df['Stochastic_%D_15'] = portfolio_df['Stochastic_5'].rolling(window = 15).mean()\n",
    "    portfolio_df['Stochastic_Ratio'] = portfolio_df['Stochastic_%D_5']/portfolio_df['Stochastic_%D_15']\n",
    "\n",
    "    # Calculate the TP,sma, mad, cci, previous_close and TR then add each to the portfolio dataframe\n",
    "    portfolio_df['TP'] = (portfolio_df['high'] + portfolio_df['low'] + portfolio_df['close']) / 3\n",
    "    portfolio_df['sma'] = portfolio_df.groupby('symbol')['TP'].transform(lambda x: x.rolling(window=26).mean())\n",
    "    portfolio_df['mad'] = portfolio_df['TP'].rolling(window=26).apply(lambda x: pd.Series(x).mad()) #Calculates Mean Absolute Deviation of 'TP' using a 21 period and returns a pandas series\n",
    "    portfolio_df['CCI'] = (portfolio_df['TP'] - portfolio_df['sma']) / (0.015 * portfolio_df['mad'])\n",
    "    portfolio_df['prev_close'] = portfolio_df.groupby('symbol')['close'].shift(1)\n",
    "    portfolio_df['Actual Returns'] = portfolio_df.groupby('symbol')['close'].pct_change()\n",
    "    portfolio_df['TR'] = np.maximum((portfolio_df['high'] - portfolio_df['low']),\n",
    "                                np.maximum(abs(portfolio_df['high'] - portfolio_df['prev_close']), \n",
    "                                abs(portfolio_df['prev_close'] - portfolio_df['low'])))\n",
    "\n",
    "    # Calculate the ATR12 and 26 and add each to the portfolio dataframe\n",
    "    for i in portfolio_df['symbol'].unique():\n",
    "        ATR_12 = []\n",
    "        ATR_26 = []\n",
    "        TR_data = portfolio_df[portfolio_df.symbol == i].copy()\n",
    "        portfolio_df.loc[portfolio_df.symbol==i,'ATR_12'] = (TR_data['TR']).rolling(window=12).mean()\n",
    "        portfolio_df.loc[portfolio_df.symbol==i,'ATR_26'] = (TR_data['TR']).rolling(window=26).mean()\n",
    "    portfolio_df['ATR_Ratio'] = portfolio_df['ATR_12'] / portfolio_df['ATR_26']\n",
    "        \n",
    "    # Reset then set the index on the dataframe then output to csv file for later use\n",
    "    portfolio_df.reset_index(inplace=True)\n",
    "    portfolio_df.set_index(['symbol', 'timestamp'], inplace=True)\n",
    "    path = (f'../Resources/portfolio_indicators_{date.today()}.csv')\n",
    "    portfolio_df.to_csv(path)\n",
    "    \n",
    "    return portfolio_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15b1c8e-d4dc-45d0-b01d-6e2ea135d883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_returns_df():\n",
    "    # Create the Alpaca API object, specifying use of the paper trading account:\n",
    "    api = create_alpaca_connection()\n",
    "    \n",
    "    # Set the list of tickers to the top class stock\n",
    "    tickers = top_class_stocks.index\n",
    "    # Set get data from API to DataFrame\n",
    "    today = datetime.now()\n",
    "    yesterday = today - timedelta(days=1)\n",
    "    previous_days = today - timedelta(days=30)\n",
    "    beg_date = previous_days\n",
    "    end_date = yesterday\n",
    "    timeframe='1Day'\n",
    "    start =  pd.Timestamp(f'{beg_date} 09:30:00-0400', tz='America/New_York').replace(hour=9, minute=30, second=0).astimezone('GMT').isoformat()[:-6]+'Z'\n",
    "    end =  pd.Timestamp(f'{end_date} 16:00:00-0400', tz='America/New_York').replace(hour=16, minute=0, second=0).astimezone('GMT').isoformat()[:-6]+'Z'\n",
    "    portfolio_df = api.get_bars(tickers, timeframe, start=start, end=end).df\n",
    "\n",
    "    # Pull prices from the ALPACA API\n",
    "    data = api.get_bars(tickers, timeframe, start=start, end=end).df\n",
    "    \n",
    "    close_df = pd.DataFrame(index=data.index)\n",
    "\n",
    "    for ticker in tickers:\n",
    "        vector = data.loc[data[\"symbol\"] == ticker].close\n",
    "        close_df[ticker] = vector\n",
    "\n",
    "    close_df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    # Use Pandas' forward fill function to fill missing values (be sure to set inplace=True)\n",
    "    close_df.ffill(inplace=True)\n",
    "    \n",
    "    # Define a variable to set prediction period\n",
    "    forecast = 1\n",
    "\n",
    "    # Compute the pct_change for 1 min \n",
    "    returns = close_df.pct_change(periods=forecast)\n",
    "\n",
    "    # Shift the returns to convert them to forward returns\n",
    "    returns = returns.shift(-(forecast))\n",
    "    returns.dropna(inplace=True)\n",
    "    path = (f'../Resources/returns_{date.today()}.csv')\n",
    "    returns.to_csv(path)\n",
    "    \n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78da219a-ba5a-468d-b0f2-afaf9f7e603b",
   "metadata": {},
   "source": [
    "## Fundemental Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0706e5-ec19-4b8c-95da-a0f3363c9a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_df = get_sentiment_df()\n",
    "sentiment_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c94d8a-c0f8-4f0b-a44e-a1faff55904c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_trending_df = get_sentiment_trending_df()\n",
    "sentiment_trending_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5a95a6-9309-4834-93c5-e4a99d5d603b",
   "metadata": {},
   "source": [
    "## Fundemental Analysis - Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364f5477-c8ae-4467-af44-d22c091ce720",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_trending_plot_df = sentiment_trending_df.set_index(\"stock\")\n",
    "sentiment_trending_plot_df[\"score\"].plot(\n",
    "    kind='bar',\n",
    "    x='stock',\n",
    "    y='score', \n",
    "    title = \"Trending Stock Sentiment Scores\",\n",
    "    figsize=(20,10)\n",
    ")\n",
    "\n",
    "#cumulative_returns.plot(figsize=(20,10), title = \"Cumulative Returns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baeb32be-2ada-44ef-8d69-a47231018dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the 'CoinName'column in its own DataFrame prior to dropping it from crypto_df\n",
    "stock_name = pd.DataFrame(sentiment_trending_df['stock'])\n",
    "stock_name.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9700b5f-08c1-47ee-b2e3-652af2c1e7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare Data\n",
    "X = sentiment_trending_df.set_index(\"stock\")\n",
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa53160-ed21-4c9f-a530-20329070bb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "X_scaled[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a44b9c-172d-4c78-ba36-4fb1c7735520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use PCA to reduce dimensions to 3 principal components\n",
    "pca = PCA(n_components=3)\n",
    "sentiment_pca = pca.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349a30aa-d030-4886-9ff5-a7e539f8b1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with the principal components data\n",
    "pcs_df = pd.DataFrame(\n",
    "    data=sentiment_pca, columns=[\"PC 1\", \"PC 2\", \"PC 3\"], index=X.index\n",
    ")\n",
    "pcs_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efa0f12-a7e4-4c3a-86f7-ca0fbe15b368",
   "metadata": {},
   "outputs": [],
   "source": [
    "inertia = []\n",
    "k = list(range(1, 11))\n",
    "\n",
    "# Calculate the inertia for the range of k values\n",
    "for i in k:\n",
    "    km = KMeans(n_clusters=i, random_state=0)\n",
    "    km.fit(pcs_df)\n",
    "    inertia.append(km.inertia_)\n",
    "\n",
    "# Create the Elbow Curve using hvPlot\n",
    "elbow_data = {\"k\": k, \"inertia\": inertia}\n",
    "df_elbow = pd.DataFrame(elbow_data)\n",
    "df_elbow.hvplot.line(x=\"k\", y=\"inertia\", title=\"Elbow Curve\", xticks=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e409d3-7c81-48bd-94a4-fce92a0f080d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the K-Means model\n",
    "model = KMeans(n_clusters=5, random_state=0)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(pcs_df)\n",
    "\n",
    "# Predict clusters\n",
    "predictions = model.predict(pcs_df)\n",
    "\n",
    "# Create a new DataFrame including predicted clusters and cryptocurrencies features\n",
    "clustered_df = pd.DataFrame({\n",
    "    \"score\": X.score,\n",
    "    \"positive_score\": X.positive_score,\n",
    "    \"negative_score\": X.negative_score,\n",
    "    \"activity\": X.activity,\n",
    "    \"activity_avg_7_days\": X.activity_avg_7_days,\n",
    "    \"activity_avg_14_days\": X.activity_avg_14_days,\n",
    "    \"activity_avg_30_days\": X.activity_avg_30_days,\n",
    "    \"score_avg_7_days\": X.score_avg_7_days,\n",
    "    \"score_avg_14_days\": X.score_avg_14_days,\n",
    "    \"score_avg_30_days\": X.score_avg_30_days,\n",
    "    \"PC 1\": pcs_df['PC 1'],\n",
    "    \"PC 2\": pcs_df['PC 2'],\n",
    "    \"PC 3\": pcs_df['PC 3'],\n",
    "    \"Class\": model.labels_,\n",
    "    },\n",
    "    index=X.index\n",
    ")\n",
    "clustered_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3391c3ee-2a5f-485e-acc2-f426a4c4f259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the 3D-Scatter with x=\"PC 1\", y=\"PC 2\" and z=\"PC 3\"\n",
    "fig = px.scatter_3d(\n",
    "    clustered_df,\n",
    "    x=\"PC 1\",\n",
    "    y=\"PC 2\",\n",
    "    z=\"PC 3\",\n",
    "    hover_name='score',\n",
    "    hover_data= ['activity'],\n",
    "    height=600,\n",
    "    color=\"Class\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd35a909-3c2d-4dc7-b377-949d4e49a3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_stocks = clustered_df.sort_values(\"score\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9940b422-5ff9-48f2-adaf-18812300a05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all stocks in the top class\n",
    "top_class = top_stocks[\"Class\"].mode()\n",
    "top_class_stocks = top_stocks.loc[top_stocks[\"Class\"] == top_class[0]]\n",
    "top_class_stocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6025d8f0-99ee-43bc-8323-ac3a22a90fdb",
   "metadata": {},
   "source": [
    "## Technical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab7fcc6-a913-4ade-ad8e-c91dca6f5478",
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_df = create_technical_analysis_df()\n",
    "portfolio_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd50a0a4-fada-44b6-91b8-9f1a4f1dd3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note this .drop function automatically moves the 'symbol' column to create a multi-level index once row 6 is dropped from original df\n",
    "technicals = portfolio_df[[\"Stochastic_Ratio\",\"CCI\",\"ATR_Ratio\",\"close\"]]\n",
    "technicals.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0343602-a36a-4b75-ac5d-ac59a5546846",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "# Machine Learning - LSTM Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe78820b-b374-4898-bf28-1a70c6e55355",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Preprocessing for LSTM\n",
    "\n",
    "Dataframe (stock_df) specified below needs to be close prices with all indicators (fundamental + technical) to be passed as features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "920f31c4-6a74-4160-89e2-9497918101f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Converting feature and targets into arrays readable by the LSTM network\n",
    "def scale_array(features, target, train_proportion:float = 0.8, scaler: bool = True):\n",
    "    \n",
    "    '''\n",
    "    Prepares four arrays for training within an LSTM neural network. \n",
    "    Returns the following objects\n",
    "    X_train: training set for features\n",
    "    X_test: testing set for features\n",
    "    y_train: training set for target(s)\n",
    "    y_test: testing set for target(s)\n",
    "    scaler: sklearn MinMaxScaler with fit memory required for inverse transformation of model predictions\n",
    "    \n",
    "    Parameters:    \n",
    "    :features: Pandas dataframe or Series object containing model features\n",
    "    :target: Pandas dataframe or Series object containing moel target(s)\n",
    "    :train: Proportion of data to be assigned to train set. The rest will be assigned to test set.\n",
    "    :scaler: Boolean. Default = True.If False, data will not be scaled.\n",
    "    '''\n",
    "    # Convert features and target to arrays\n",
    "    X = np.array(features)\n",
    "    y = np.array(target).reshape(-1,1)\n",
    "    \n",
    "    # Manually splitting the data\n",
    "    split = int(0.8 * len(X))\n",
    "    X_train = X[: split]\n",
    "    X_test = X[split:]\n",
    "    y_train = y[: split]\n",
    "    y_test = y[split:]\n",
    "\n",
    "    if scaler:\n",
    "        # Create a MinMaxScaler object\n",
    "        scaler = MinMaxScaler()\n",
    "\n",
    "        # Fit the MinMaxScaler object with the features data X\n",
    "        scaler.fit(X_train)\n",
    "\n",
    "        # Scale the features training and testing sets\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # Fit the MinMaxScaler object with the target data Y\n",
    "        scaler.fit(y_train)\n",
    "\n",
    "        # Scale the target training and testing sets\n",
    "        y_train = scaler.transform(y_train)\n",
    "        y_test = scaler.transform(y_test)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    # Reshape the features data to pass into LSTM\n",
    "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, scaler\n",
    "\n",
    "# Creating the LSTM neural network layout\n",
    "\n",
    "def create_LSTM_model(\n",
    "    train_set: np.ndarray,\n",
    "    dropout: float = 0.2,\n",
    "    layer_one_dropout: float = 0.6,\n",
    "    number_layers: int = 4,\n",
    "    optimizer: str = 'adam',\n",
    "    loss: str = 'mean_squared_error'):\n",
    "    \n",
    "    '''\n",
    "    Initialises a multilayer LSTM neural network, with number of units in the first layer being equal to the number of features. Number of layers is default 4, but can be specified by user.\n",
    "    Each layer is accompanied by a Dropout with a rate of 0.6 for the first layer and a default of 0.2 for subsequent layers.\n",
    "    After the first layer, number of units in each LSTM are reduced to 2/3 the initial size.\n",
    "    '''\n",
    "\n",
    "    # Define the LSTM RNN model.\n",
    "    model = Sequential()\n",
    "\n",
    "    # Initial model setup\n",
    "    number_units = X_train.shape[1]\n",
    "    dropout_fraction = dropout\n",
    "\n",
    "\n",
    "    # Layer 1\n",
    "    model.add(LSTM(\n",
    "        units=number_units,\n",
    "        return_sequences=True,\n",
    "        input_shape=(X_train.shape[1], 1))\n",
    "        )\n",
    "    model.add(Dropout(layer_one_dropout))\n",
    "\n",
    "    # Intiialize layer counter\n",
    "    layer_counter = 1\n",
    "    \n",
    "    # 'While' loop to keep adding layers until number of layers meet user specifications. Condition is \"< - 1\" because of need for penultimate layer not to have \"return_sequences = True\".\n",
    "    while layer_counter < (number_layers - 1):\n",
    "        # Layer 2 to n\n",
    "        model.add(LSTM(units=number_units, return_sequences = True))\n",
    "        model.add(Dropout(dropout_fraction))\n",
    "        layer_counter+=1\n",
    "\n",
    "    # Penultimate layer\n",
    "    model.add(LSTM(units=number_units))\n",
    "    model.add(Dropout(dropout_fraction))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer, loss=loss)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Function to calculate returns according to strategy, and reappend to existing prices dataframe\n",
    "def calculate_strategy_returns(prices_df, trading_threshold, shorting: bool = False):\n",
    "    '''\n",
    "    prices_df: pd.DataFrame containing an 'Actual' and 'Predicted' column representing actual and model-predicted prices respectively\n",
    "    \n",
    "    '''\n",
    "     # Calculate actual daily returns\n",
    "    prices_df['actual_returns'] = prices_df['Actual'].pct_change()\n",
    "    # Create a 'last close' column\n",
    "    prices_df['last_close'] = prices_df['Actual'].shift()\n",
    "    # Calculate the predicted daily returns, by taking the predicted price as a proportion of the last close\n",
    "    prices_df['predicted_returns'] = (prices_df['Predicted'] - prices_df['last_close'])/prices_df['last_close']\n",
    "\n",
    "    # Actual signal = 1 if actual returns more than threshold,  -1 if less than threshold\n",
    "    prices_df['actual_signal'] = 0\n",
    "    prices_df.loc[prices_df['actual_returns'] > trading_threshold , 'actual_signal'] = 1\n",
    "    if shorting == True:\n",
    "        prices_df.loc[prices_df['actual_returns'] < -trading_threshold , 'actual_signal'] = -1\n",
    "\n",
    "    # Strategy signal = 1 if predicted returns > threshold, -1 if less than threshold\n",
    "    prices_df['strategy_signal'] = 0\n",
    "    prices_df.loc[prices_df['predicted_returns'] > trading_threshold , 'strategy_signal'] = 1\n",
    "    if shorting == True:\n",
    "        prices_df.loc[prices_df['predicted_returns'] < -trading_threshold , 'strategy_signal'] = -1       \n",
    "\n",
    "    # Compute strategy returns\n",
    "    prices_df['strategy_returns'] = prices_df['actual_returns'] * prices_df['strategy_signal']\n",
    "    \n",
    "    return prices_df\n",
    "\n",
    "\n",
    "# Function to calculate RMSE. 'math` library needed. \n",
    "def calculate_RMSE(y_actual, y_predicted):\n",
    "    MSE = np.square(np.subtract(y_actual, y_predicted)).mean()\n",
    "    RMSE = math.sqrt(MSE)\n",
    "    return RMSE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcd794c-c45b-4bfa-ac82-82b0a7b9776c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Predefine parameters for LSTM network training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e193452d-4409-43cb-82f1-06595833b0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set target cumulative returns as a threshold for model to achieve.\n",
    "target_cumulative_return = 1.01\n",
    "\n",
    "# Set returns threshold for strategy to fire trading signal\n",
    "trading_threshold = 0.00\n",
    "\n",
    "# Set maximum numberof iterations to run\n",
    "max_iter = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bd9543-a66e-49cb-8513-b8fc21f98d07",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "### LSTM `for` loop to train models for each candidate ticker\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "db71f590-6ee1-4e28-b195-f055656c7034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Initialising training for ADBE\n",
      "==================================================\n",
      "Training ADBE model iteration 1 ...please wait.\n",
      "\n",
      "Iteration 1 ended.\n",
      "LSTM Method iteration 1 for ADBE - Performance\n",
      "--------------------------------------------------\n",
      "Model loss on testing dataset: \n",
      "0.2099\n",
      "RMSE: \n",
      "99.0676\n",
      "Cumulative return on testing dataset: \n",
      "0.8533\n",
      "==================================================\n",
      "Training ADBE model iteration 2 ...please wait.\n",
      "\n",
      "Iteration 2 ended.\n",
      "LSTM Method iteration 2 for ADBE - Performance\n",
      "--------------------------------------------------\n",
      "Model loss on testing dataset: \n",
      "0.2079\n",
      "RMSE: \n",
      "98.6081\n",
      "Cumulative return on testing dataset: \n",
      "0.8533\n",
      "==================================================\n",
      "Training ADBE model iteration 3 ...please wait.\n",
      "\n",
      "Iteration 3 ended.\n",
      "LSTM Method iteration 3 for ADBE - Performance\n",
      "--------------------------------------------------\n",
      "Model loss on testing dataset: \n",
      "0.2139\n",
      "RMSE: \n",
      "100.0215\n",
      "Cumulative return on testing dataset: \n",
      "0.8533\n",
      "The LSTM model was not able to achieve the target cumulative returns on the testing dataset within 3 iterations.\n",
      "\n",
      "==================================================\n",
      "Initialising training for BCS\n",
      "==================================================\n",
      "Training BCS model iteration 1 ...please wait.\n",
      "\n",
      "Iteration 1 ended.\n",
      "LSTM Method iteration 1 for BCS - Performance\n",
      "--------------------------------------------------\n",
      "Model loss on testing dataset: \n",
      "0.2349\n",
      "RMSE: \n",
      "1.2019\n",
      "Cumulative return on testing dataset: \n",
      "0.8887\n",
      "==================================================\n",
      "Training BCS model iteration 2 ...please wait.\n",
      "\n",
      "Iteration 2 ended.\n",
      "LSTM Method iteration 2 for BCS - Performance\n",
      "--------------------------------------------------\n",
      "Model loss on testing dataset: \n",
      "0.2288\n",
      "RMSE: \n",
      "1.1863\n",
      "Cumulative return on testing dataset: \n",
      "0.8887\n",
      "==================================================\n",
      "Training BCS model iteration 3 ...please wait.\n",
      "\n",
      "Iteration 3 ended.\n",
      "LSTM Method iteration 3 for BCS - Performance\n",
      "--------------------------------------------------\n",
      "Model loss on testing dataset: \n",
      "0.2337\n",
      "RMSE: \n",
      "1.1988\n",
      "Cumulative return on testing dataset: \n",
      "0.8887\n",
      "The LSTM model was not able to achieve the target cumulative returns on the testing dataset within 3 iterations.\n",
      "\n",
      "==================================================\n",
      "Initialising training for CRON\n",
      "==================================================\n",
      "Training CRON model iteration 1 ...please wait.\n",
      "\n",
      "Iteration 1 ended.\n",
      "LSTM Method iteration 1 for CRON - Performance\n",
      "--------------------------------------------------\n",
      "Model loss on testing dataset: \n",
      "0.0141\n",
      "RMSE: \n",
      "0.6182\n",
      "Cumulative return on testing dataset: \n",
      "0.8420\n",
      "==================================================\n",
      "Training CRON model iteration 2 ...please wait.\n",
      "\n",
      "Iteration 2 ended.\n",
      "LSTM Method iteration 2 for CRON - Performance\n",
      "--------------------------------------------------\n",
      "Model loss on testing dataset: \n",
      "0.0130\n",
      "RMSE: \n",
      "0.5930\n",
      "Cumulative return on testing dataset: \n",
      "0.8420\n",
      "==================================================\n",
      "Training CRON model iteration 3 ...please wait.\n",
      "\n",
      "Iteration 3 ended.\n",
      "LSTM Method iteration 3 for CRON - Performance\n",
      "--------------------------------------------------\n",
      "Model loss on testing dataset: \n",
      "0.0148\n",
      "RMSE: \n",
      "0.6331\n",
      "Cumulative return on testing dataset: \n",
      "0.8420\n",
      "The LSTM model was not able to achieve the target cumulative returns on the testing dataset within 3 iterations.\n",
      "\n",
      "==================================================\n",
      "Initialising training for CRWD\n",
      "==================================================\n",
      "Training CRWD model iteration 1 ...please wait.\n",
      "\n",
      "Iteration 1 ended.\n",
      "LSTM Method iteration 1 for CRWD - Performance\n",
      "--------------------------------------------------\n",
      "Model loss on testing dataset: \n",
      "0.0720\n",
      "RMSE: \n",
      "29.3602\n",
      "Cumulative return on testing dataset: \n",
      "1.0654\n",
      "Target cumulative returns achieved\n",
      "\n",
      "From 2022-01-05 05:00:00+00:00 to 2022-03-10 05:01:00+00:00, the cumulative return of the current model is 1.07.\n",
      "At its lowest, the model recorded a cumulative return of 0.87.\n",
      "At its highest, the model recorded a cumulative return of 1.13.\n",
      "==================================================\n",
      "Initialising training for DOCU\n",
      "==================================================\n",
      "Training DOCU model iteration 1 ...please wait.\n",
      "\n",
      "Iteration 1 ended.\n",
      "LSTM Method iteration 1 for DOCU - Performance\n",
      "--------------------------------------------------\n",
      "Model loss on testing dataset: \n",
      "0.0382\n",
      "RMSE: \n",
      "34.1825\n",
      "Cumulative return on testing dataset: \n",
      "0.6512\n",
      "==================================================\n",
      "Training DOCU model iteration 2 ...please wait.\n",
      "\n",
      "Iteration 2 ended.\n",
      "LSTM Method iteration 2 for DOCU - Performance\n",
      "--------------------------------------------------\n",
      "Model loss on testing dataset: \n",
      "0.0412\n",
      "RMSE: \n",
      "35.4939\n",
      "Cumulative return on testing dataset: \n",
      "0.6512\n",
      "==================================================\n",
      "Training DOCU model iteration 3 ...please wait.\n",
      "\n",
      "Iteration 3 ended.\n",
      "LSTM Method iteration 3 for DOCU - Performance\n",
      "--------------------------------------------------\n",
      "Model loss on testing dataset: \n",
      "0.0408\n",
      "RMSE: \n",
      "35.3287\n",
      "Cumulative return on testing dataset: \n",
      "0.6512\n",
      "The LSTM model was not able to achieve the target cumulative returns on the testing dataset within 3 iterations.\n",
      "\n",
      "==================================================\n",
      "Initialising training for FNKO\n",
      "==================================================\n",
      "Training FNKO model iteration 1 ...please wait.\n",
      "\n",
      "Iteration 1 ended.\n",
      "LSTM Method iteration 1 for FNKO - Performance\n",
      "--------------------------------------------------\n",
      "Model loss on testing dataset: \n",
      "0.0058\n",
      "RMSE: \n",
      "0.8143\n",
      "Cumulative return on testing dataset: \n",
      "1.1251\n",
      "Target cumulative returns achieved\n",
      "\n",
      "From 2022-01-05 05:00:00+00:00 to 2022-03-10 05:01:00+00:00, the cumulative return of the current model is 1.13.\n",
      "At its lowest, the model recorded a cumulative return of 0.92.\n",
      "At its highest, the model recorded a cumulative return of 1.19.\n",
      "==================================================\n",
      "Initialising training for GME\n",
      "==================================================\n",
      "Training GME model iteration 1 ...please wait.\n",
      "\n",
      "Iteration 1 ended.\n",
      "LSTM Method iteration 1 for GME - Performance\n",
      "--------------------------------------------------\n",
      "Model loss on testing dataset: \n",
      "0.0550\n",
      "RMSE: \n",
      "38.8467\n",
      "Cumulative return on testing dataset: \n",
      "0.7773\n",
      "==================================================\n",
      "Training GME model iteration 2 ...please wait.\n",
      "\n",
      "Iteration 2 ended.\n",
      "LSTM Method iteration 2 for GME - Performance\n",
      "--------------------------------------------------\n",
      "Model loss on testing dataset: \n",
      "0.0528\n",
      "RMSE: \n",
      "38.0882\n",
      "Cumulative return on testing dataset: \n",
      "0.7773\n",
      "==================================================\n",
      "Training GME model iteration 3 ...please wait.\n",
      "\n",
      "Iteration 3 ended.\n",
      "LSTM Method iteration 3 for GME - Performance\n",
      "--------------------------------------------------\n",
      "Model loss on testing dataset: \n",
      "0.0494\n",
      "RMSE: \n",
      "36.8320\n",
      "Cumulative return on testing dataset: \n",
      "0.7773\n",
      "The LSTM model was not able to achieve the target cumulative returns on the testing dataset within 3 iterations.\n",
      "\n",
      "==================================================\n",
      "Initialising training for NIO\n",
      "==================================================\n",
      "Training NIO model iteration 1 ...please wait.\n",
      "\n",
      "Iteration 1 ended.\n",
      "LSTM Method iteration 1 for NIO - Performance\n",
      "--------------------------------------------------\n",
      "Model loss on testing dataset: \n",
      "0.0946\n",
      "RMSE: \n",
      "7.7031\n",
      "Cumulative return on testing dataset: \n",
      "0.6181\n",
      "==================================================\n",
      "Training NIO model iteration 2 ...please wait.\n",
      "\n",
      "Iteration 2 ended.\n",
      "LSTM Method iteration 2 for NIO - Performance\n",
      "--------------------------------------------------\n",
      "Model loss on testing dataset: \n",
      "0.0916\n",
      "RMSE: \n",
      "7.5803\n",
      "Cumulative return on testing dataset: \n",
      "0.6181\n",
      "==================================================\n",
      "Training NIO model iteration 3 ...please wait.\n",
      "\n",
      "Iteration 3 ended.\n",
      "LSTM Method iteration 3 for NIO - Performance\n",
      "--------------------------------------------------\n",
      "Model loss on testing dataset: \n",
      "0.0918\n",
      "RMSE: \n",
      "7.5863\n",
      "Cumulative return on testing dataset: \n",
      "0.6181\n",
      "The LSTM model was not able to achieve the target cumulative returns on the testing dataset within 3 iterations.\n",
      "\n",
      "==================================================\n",
      "Initialising training for NKLA\n",
      "==================================================\n",
      "Training NKLA model iteration 1 ...please wait.\n",
      "\n",
      "Iteration 1 ended.\n",
      "LSTM Method iteration 1 for NKLA - Performance\n",
      "--------------------------------------------------\n",
      "Model loss on testing dataset: \n",
      "0.0430\n",
      "RMSE: \n",
      "2.0407\n",
      "Cumulative return on testing dataset: \n",
      "0.8912\n",
      "==================================================\n",
      "Training NKLA model iteration 2 ...please wait.\n",
      "\n",
      "Iteration 2 ended.\n",
      "LSTM Method iteration 2 for NKLA - Performance\n",
      "--------------------------------------------------\n",
      "Model loss on testing dataset: \n",
      "0.0441\n",
      "RMSE: \n",
      "2.0667\n",
      "Cumulative return on testing dataset: \n",
      "0.9648\n",
      "==================================================\n",
      "Training NKLA model iteration 3 ...please wait.\n",
      "\n",
      "Iteration 3 ended.\n",
      "LSTM Method iteration 3 for NKLA - Performance\n",
      "--------------------------------------------------\n",
      "Model loss on testing dataset: \n",
      "0.0439\n",
      "RMSE: \n",
      "2.0619\n",
      "Cumulative return on testing dataset: \n",
      "0.9648\n",
      "The LSTM model was not able to achieve the target cumulative returns on the testing dataset within 3 iterations.\n",
      "\n",
      "==================================================\n",
      "Initialising training for ORCL\n",
      "==================================================\n",
      "Training ORCL model iteration 1 ...please wait.\n",
      "\n",
      "Iteration 1 ended.\n",
      "LSTM Method iteration 1 for ORCL - Performance\n",
      "--------------------------------------------------\n",
      "Model loss on testing dataset: \n",
      "0.0690\n",
      "RMSE: \n",
      "7.6552\n",
      "Cumulative return on testing dataset: \n",
      "0.9068\n",
      "==================================================\n",
      "Training ORCL model iteration 2 ...please wait.\n",
      "\n",
      "Iteration 2 ended.\n",
      "LSTM Method iteration 2 for ORCL - Performance\n",
      "--------------------------------------------------\n",
      "Model loss on testing dataset: \n",
      "0.0641\n",
      "RMSE: \n",
      "7.3775\n",
      "Cumulative return on testing dataset: \n",
      "0.9081\n",
      "==================================================\n",
      "Training ORCL model iteration 3 ...please wait.\n",
      "\n",
      "Iteration 3 ended.\n",
      "LSTM Method iteration 3 for ORCL - Performance\n",
      "--------------------------------------------------\n",
      "Model loss on testing dataset: \n",
      "0.0610\n",
      "RMSE: \n",
      "7.1981\n",
      "Cumulative return on testing dataset: \n",
      "0.8960\n",
      "The LSTM model was not able to achieve the target cumulative returns on the testing dataset within 3 iterations.\n",
      "\n",
      "==================================================\n",
      "Initialising training for QCOM\n",
      "==================================================\n",
      "Training QCOM model iteration 1 ...please wait.\n",
      "\n",
      "Iteration 1 ended.\n",
      "LSTM Method iteration 1 for QCOM - Performance\n",
      "--------------------------------------------------\n",
      "Model loss on testing dataset: \n",
      "0.1577\n",
      "RMSE: \n",
      "26.3422\n",
      "Cumulative return on testing dataset: \n",
      "1.0000\n",
      "==================================================\n",
      "Training QCOM model iteration 2 ...please wait.\n",
      "\n",
      "Iteration 2 ended.\n",
      "LSTM Method iteration 2 for QCOM - Performance\n",
      "--------------------------------------------------\n",
      "Model loss on testing dataset: \n",
      "0.1607\n",
      "RMSE: \n",
      "26.5914\n",
      "Cumulative return on testing dataset: \n",
      "1.0000\n",
      "==================================================\n",
      "Training QCOM model iteration 3 ...please wait.\n",
      "\n",
      "Iteration 3 ended.\n",
      "LSTM Method iteration 3 for QCOM - Performance\n",
      "--------------------------------------------------\n",
      "Model loss on testing dataset: \n",
      "0.1629\n",
      "RMSE: \n",
      "26.7679\n",
      "Cumulative return on testing dataset: \n",
      "1.0000\n",
      "The LSTM model was not able to achieve the target cumulative returns on the testing dataset within 3 iterations.\n",
      "\n",
      "==================================================\n",
      "Initialising training for RACE\n",
      "==================================================\n",
      "Training RACE model iteration 1 ...please wait.\n",
      "\n",
      "Iteration 1 ended.\n",
      "LSTM Method iteration 1 for RACE - Performance\n",
      "--------------------------------------------------\n",
      "Model loss on testing dataset: \n",
      "0.0573\n",
      "RMSE: \n",
      "19.0917\n",
      "Cumulative return on testing dataset: \n",
      "0.8823\n",
      "==================================================\n",
      "Training RACE model iteration 2 ...please wait.\n",
      "\n",
      "Iteration 2 ended.\n",
      "LSTM Method iteration 2 for RACE - Performance\n",
      "--------------------------------------------------\n",
      "Model loss on testing dataset: \n",
      "0.0577\n",
      "RMSE: \n",
      "19.1517\n",
      "Cumulative return on testing dataset: \n",
      "0.9086\n",
      "==================================================\n",
      "Training RACE model iteration 3 ...please wait.\n",
      "\n",
      "Iteration 3 ended.\n",
      "LSTM Method iteration 3 for RACE - Performance\n",
      "--------------------------------------------------\n",
      "Model loss on testing dataset: \n",
      "0.0573\n",
      "RMSE: \n",
      "19.0847\n",
      "Cumulative return on testing dataset: \n",
      "0.8884\n",
      "The LSTM model was not able to achieve the target cumulative returns on the testing dataset within 3 iterations.\n",
      "\n",
      "==================================================\n",
      "Initialising training for RMBL\n",
      "==================================================\n",
      "Training RMBL model iteration 1 ...please wait.\n",
      "\n",
      "Iteration 1 ended.\n",
      "LSTM Method iteration 1 for RMBL - Performance\n",
      "--------------------------------------------------\n",
      "Model loss on testing dataset: \n",
      "0.2028\n",
      "RMSE: \n",
      "7.5204\n",
      "Cumulative return on testing dataset: \n",
      "1.1894\n",
      "Target cumulative returns achieved\n",
      "\n",
      "From 2022-01-05 05:00:00+00:00 to 2022-03-10 05:01:00+00:00, the cumulative return of the current model is 1.19.\n",
      "At its lowest, the model recorded a cumulative return of 0.87.\n",
      "At its highest, the model recorded a cumulative return of 1.19.\n",
      "==================================================\n",
      "Initialising training for SHOP\n",
      "==================================================\n",
      "Training SHOP model iteration 1 ...please wait.\n",
      "\n",
      "Iteration 1 ended.\n",
      "LSTM Method iteration 1 for SHOP - Performance\n",
      "--------------------------------------------------\n",
      "Model loss on testing dataset: \n",
      "0.7496\n",
      "RMSE: \n",
      "556.5645\n",
      "Cumulative return on testing dataset: \n",
      "0.4649\n",
      "==================================================\n",
      "Training SHOP model iteration 2 ...please wait.\n",
      "\n",
      "Iteration 2 ended.\n",
      "LSTM Method iteration 2 for SHOP - Performance\n",
      "--------------------------------------------------\n",
      "Model loss on testing dataset: \n",
      "0.7487\n",
      "RMSE: \n",
      "556.2182\n",
      "Cumulative return on testing dataset: \n",
      "0.4649\n",
      "==================================================\n",
      "Training SHOP model iteration 3 ...please wait.\n",
      "\n",
      "Iteration 3 ended.\n",
      "LSTM Method iteration 3 for SHOP - Performance\n",
      "--------------------------------------------------\n",
      "Model loss on testing dataset: \n",
      "0.7551\n",
      "RMSE: \n",
      "558.5931\n",
      "Cumulative return on testing dataset: \n",
      "0.4649\n",
      "The LSTM model was not able to achieve the target cumulative returns on the testing dataset within 3 iterations.\n",
      "\n",
      "==================================================\n",
      "Initialising training for SPY\n",
      "==================================================\n",
      "Training SPY model iteration 1 ...please wait.\n",
      "\n",
      "Iteration 1 ended.\n",
      "LSTM Method iteration 1 for SPY - Performance\n",
      "--------------------------------------------------\n",
      "Model loss on testing dataset: \n",
      "0.0432\n",
      "RMSE: \n",
      "15.0255\n",
      "Cumulative return on testing dataset: \n",
      "1.0105\n",
      "Target cumulative returns achieved\n",
      "\n",
      "From 2022-01-05 05:00:00+00:00 to 2022-03-10 05:01:00+00:00, the cumulative return of the current model is 1.01.\n",
      "At its lowest, the model recorded a cumulative return of 0.99.\n",
      "At its highest, the model recorded a cumulative return of 1.04.\n",
      "==================================================\n",
      "Initialising training for TFII\n",
      "==================================================\n",
      "Training TFII model iteration 1 ...please wait.\n",
      "\n",
      "Iteration 1 ended.\n",
      "LSTM Method iteration 1 for TFII - Performance\n",
      "--------------------------------------------------\n",
      "Model loss on testing dataset: \n",
      "0.0120\n",
      "RMSE: \n",
      "4.7020\n",
      "Cumulative return on testing dataset: \n",
      "1.0661\n",
      "Target cumulative returns achieved\n",
      "\n",
      "From 2022-01-05 05:00:00+00:00 to 2022-03-10 05:01:00+00:00, the cumulative return of the current model is 1.07.\n",
      "At its lowest, the model recorded a cumulative return of 0.93.\n",
      "At its highest, the model recorded a cumulative return of 1.07.\n",
      "**************************************************\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "tickers = technicals.index.get_level_values('symbol').unique().to_list()\n",
    "\n",
    "# Initialise list to hold tickers that have successfully trained models that achieve the target cumulative returns:\n",
    "modelled_tickers = []\n",
    "trading_signals = []\n",
    "\n",
    "for ticker in tickers:\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Initialising training for {ticker}\")\n",
    "\n",
    "    # Create signal dataframe as a copy\n",
    "    signal = technicals.copy().loc[ticker].dropna()\n",
    "    \n",
    "    # Create blank row for current trading day and append to end of dataframe\n",
    "    most_recent_timestamp = signal.index.get_level_values('timestamp').max() + timedelta(minutes = 1)\n",
    "    signal.loc[most_recent_timestamp, ['target']] = np.nan\n",
    "\n",
    "    # # Create target\n",
    "    signal['target'] = signal['close'] \n",
    "\n",
    "    # Shift indicators to predict current trading day close\n",
    "    signal.iloc[:, :-1]  = signal.iloc[:, :-1].shift()\n",
    "\n",
    "    # Drop first row with NaNs resulting from data shift\n",
    "    signal = signal.iloc[1:, :]\n",
    "\n",
    "    # Ensure all data is 'float' type while also dropping null values due to value shifts and unavailable NaN indicator data.\n",
    "    signal = signal.astype('float')\n",
    "\n",
    "    # Set features and target\n",
    "    X = signal.iloc[:, :-1]\n",
    "    y = signal['target']\n",
    "      \n",
    "    # Use predefined scale_array function to transform data and perform train/test split\n",
    "    X_train, X_test, y_train, y_test, scaler = scale_array(X, y, train_proportion = 0.8)\n",
    "\n",
    "    # Record start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # (Re)set iter_counter and strategy_cumulative_return to 0 \n",
    "    strategy_cumulative_return = 0\n",
    "    iter_counter = 0\n",
    "\n",
    "    # While loop that repeatedly trains LSTM models to adjust weights until it can hit the target cumulative return. Loop stops if max_iter is hit or if returns are achieved on backtesting\n",
    "    while strategy_cumulative_return < target_cumulative_return and iter_counter != max_iter:\n",
    "        \n",
    "        strategy_cumulative_return = 0\n",
    "        # Start iteration counter\n",
    "        iter_counter+=1\n",
    "\n",
    "        # Create model if first iteration. Reset model if subsequent iterations\n",
    "        model = create_LSTM_model(X_train,\n",
    "                                  dropout=0.4,\n",
    "                                  layer_one_dropout=0.6,\n",
    "                                  number_layers=6\n",
    "                                 )\n",
    "\n",
    "        # Set early stopping such that each iteration stops running epochs if validation loss is not improving (i.e. minimising further)\n",
    "        callback = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=20, mode='auto',\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "\n",
    "        # Print message to allow visual confirmation of iteration training is currently at.\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Training {ticker} model iteration {iter_counter} ...please wait.\\n\")\n",
    "\n",
    "        # Train the model\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=1000, batch_size=32,\n",
    "            shuffle=False,\n",
    "            validation_split = 0.1,  \n",
    "            verbose = 0,\n",
    "            callbacks = callback\n",
    "        )\n",
    "        # Print confirmation that current iteration has ended.\n",
    "        print(f\"Iteration {iter_counter} ended.\")\n",
    "\n",
    "        # Evaluate loss when predicting test data. Sliced out entry -1 as y_test[-1] target is NaN \n",
    "        model_loss = model.evaluate(X_test[:-1], y_test[:-1], verbose=0)\n",
    "    \n",
    "        # Make predictions\n",
    "        predicted = model.predict(X_test)\n",
    "\n",
    "        # Recover the original prices instead of the scaled version\n",
    "        predicted_prices = scaler.inverse_transform(predicted)\n",
    "        real_prices = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "        # Create a DataFrame of Real and Predicted values\n",
    "        prices = pd.DataFrame({\n",
    "            \"Actual\": real_prices.ravel(),\n",
    "            \"Predicted\": predicted_prices.ravel()\n",
    "        }, index = signal.index[-len(real_prices): ]) \n",
    "\n",
    "        # Use predefined calculate_strategy_returns function to calculate and append strategy returns column to 'prices' dataframe\n",
    "        prices = calculate_strategy_returns(prices, trading_threshold, shorting = False)\n",
    "        \n",
    "        \n",
    "        # Compute strategy cumulative returns\n",
    "        strategy_cumulative_return = (1+prices['strategy_returns']).cumprod()[-1]\n",
    "        \n",
    "        rmse = calculate_RMSE(prices['Actual'], prices['Predicted'])\n",
    "        \n",
    "        # Print performance metrics of the model given the feature weights produced by current iteration\n",
    "        print(f\"LSTM Method iteration {iter_counter} for {ticker} - Performance\")\n",
    "        print(\"-\"*50)\n",
    "        print(f\"Model loss on testing dataset: \\n{model_loss:.4f}\")\n",
    "        print(f\"RMSE: \\n{rmse:.4f}\")\n",
    "        print(f\"Cumulative return on testing dataset: \\n{strategy_cumulative_return:.4f}\")\n",
    "    \n",
    "    # Append ticker to modelled_tickers:\n",
    "    modelled_tickers.append(ticker)\n",
    "    \n",
    "    if strategy_cumulative_return >= target_cumulative_return:\n",
    "        print(f\"Target cumulative returns achieved\\n\")\n",
    "        # Calculate cumulative returns at their best and worst time points over time.\n",
    "        min_return = (1+prices['strategy_returns']).cumprod().min()\n",
    "        max_return = (1+prices['strategy_returns']).cumprod().max()\n",
    "\n",
    "        \n",
    "        # Print cumulative return performance\n",
    "        print(f\"From {prices.index.min()} to {prices.index.max()}, the cumulative return of the current model is {strategy_cumulative_return:.2f}.\")\n",
    "        print(f\"At its lowest, the model recorded a cumulative return of {min_return:.2f}.\")\n",
    "        print(f\"At its highest, the model recorded a cumulative return of {max_return:.2f}.\")  \n",
    "        \n",
    "        # Convert model to json\n",
    "        model_json = model.to_json()\n",
    "\n",
    "        # Save model layout as json\n",
    "        file_path = Path(f\"../LSTM_model_weights/{ticker}.json\")\n",
    "        with open(file_path, \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "\n",
    "        # Save weights\n",
    "        model.save_weights(f\"../LSTM_model_weights/{ticker}.h5\")\n",
    "        \n",
    "        # Append the trading signal predicted by model\n",
    "        trading_signals.append(prices.loc[prices.index.max(), 'strategy_signal'])\n",
    "\n",
    "    else:\n",
    "        print(f\"The LSTM model was not able to achieve the target cumulative returns on the testing dataset within {max_iter} iterations.\\n\")\n",
    "        trading_signals.append(0)\n",
    "\n",
    "\n",
    "print(\"*\"*50)\n",
    "print(f\"Training completed.\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3d9490a2-63f2-49ac-bf81-1e2da705e794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trading_signal</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>symbol</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ADBE</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BCS</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRON</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRWD</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOCU</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FNKO</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GME</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NIO</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NKLA</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORCL</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QCOM</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RACE</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMBL</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SHOP</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SPY</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TFII</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        trading_signal\n",
       "symbol                \n",
       "ADBE                 0\n",
       "BCS                  0\n",
       "CRON                 0\n",
       "CRWD                 1\n",
       "DOCU                 0\n",
       "FNKO                 1\n",
       "GME                  0\n",
       "NIO                  0\n",
       "NKLA                 0\n",
       "ORCL                 0\n",
       "QCOM                 0\n",
       "RACE                 0\n",
       "RMBL                 1\n",
       "SHOP                 0\n",
       "SPY                  1\n",
       "TFII                 1"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Produce trading_signal df.\n",
    "\n",
    "trading_dict = {'symbol':modelled_tickers, 'trading_signal':trading_signals}\n",
    "trading_signals_df = pd.DataFrame.from_dict(trading_dict).set_index('symbol')\n",
    "\n",
    "trading_signals_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9cf1fa-a731-42bf-8d38-b217bcdfac0c",
   "metadata": {},
   "source": [
    "## Graphs\n",
    "\n",
    "Once we have a model that generates the desired cumulative returns, we can print the graph for further visual confirmation that this is a suitable algo. \n",
    "\n",
    "Three graphs here \n",
    "- The loss metric from the training history of the eligible model\n",
    "- Predicted prices vs Actual prices\n",
    "- Strategy cumprod vs Actual cumprod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dae687-af7d-4530-b344-eb5b3f4b3e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot validation loss versus training loss\n",
    "\n",
    "plt.plot(history.history['loss'], 'r', label='Training loss')\n",
    "plt.plot(history.history['val_loss'], 'g', label='Validation loss')\n",
    "plt.title('Training VS Validation loss')\n",
    "plt.xlabel('No. of Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cf8bdb-af5e-412a-b957-6fc4eb7b1a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the real vs predicted prices as a line chart\n",
    "price_fig = px.line(prices, y = ['Actual', 'Predicted'],  title = \"Actual vs Predicted\", width= 1500, height = 600)\n",
    "price_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e3bd99-0b48-4e2e-a7e8-35d911a135ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot strategy cumulative returns\n",
    "strategy_cumulative_returns = (1+prices['strategy_returns']).cumprod()\n",
    "actual_cumulative_returns = (1+prices['actual_returns']).cumprod()\n",
    "cumulative_returns_df = pd.concat([strategy_cumulative_returns, actual_cumulative_returns], join = \"inner\", axis = \"columns\")\n",
    "\n",
    "cumulative_returns_fig = px.line(\n",
    "    cumulative_returns_df,\n",
    "    y = ['strategy_returns', 'actual_returns'],\n",
    "    x = cumulative_returns_df.index.values,\n",
    "    title = f'Strategy  vs Actual Returns',\n",
    "    width = 1500, height = 600\n",
    ")\n",
    "\n",
    "cumulative_returns_fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a488c89-101a-44d9-9804-d780d22afe96",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## Forward Prediction and Trading Signals given saved model weights\n",
    "\n",
    "### Model Persistence (Load)\n",
    "\n",
    "Note that I've set up the file name here to save as (ticker_name).json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971345d9-eb3f-4d8c-80a1-4b4e7d0f101c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load json and create mdoel\n",
    "file_path = Path(f\"../LSTM_model_weights/{ticker}.json\")\n",
    "with open (file_path, \"r\") as json_file:\n",
    "    model_json = json_file.read()\n",
    "loaded_model = model_from_json(model_json)\n",
    "\n",
    "# Load weights into new model\n",
    "file_path = f\"../LSTM_model_weights/{ticker}.h5\"\n",
    "loaded_model.load_weights(file_path)\n",
    "\n",
    "# Visual confirmation of model setup\n",
    "print(loaded_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9b5c2d-e550-4ba9-953d-b197f60e4fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with model\n",
    "predicted = loaded_model.predict(X_test)\n",
    "predicted_prices = scaler.inverse_transform(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e639781a-e34e-480f-b200-d2cb440d520d",
   "metadata": {},
   "source": [
    "### Develop the Algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0de31cf-ffbb-415e-afe2-622d7a296086",
   "metadata": {},
   "source": [
    "#### Use the provided code to ping the Alpaca API and create the DataFrame needed to feed data into the model.\n",
    "   * This code will also store the correct feature data in `X` for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180ffcbb-62da-45aa-87c7-38dbec35618a",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_class_stocks_buy = pd.DataFrame(index=top_class_stocks.index)\n",
    "top_class_stocks_buy[\"buy\"] = [1,1,0,1,1,1,1,0,0,1,1,0,1,1,1,0,1,0,0,1,0,1,0,0,1]\n",
    "top_class_stocks_buy = top_class_stocks_buy.loc[top_class_stocks_buy[\"buy\"] == 1]\n",
    "top_class_stocks_buy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7250b37f-2bde-46b1-bdbc-a91652c0acbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tickers for top class stocks from FA\n",
    "tickers = top_class_stocks_buy.index\n",
    "\n",
    "# declare begin and end date strings\n",
    "beg_date = '2022-03-08'\n",
    "end_date = '2022-03-08'\n",
    "# we convert begin and end date to formats that the ALPACA API requires\n",
    "start =  pd.Timestamp(f'{beg_date} 09:30:00-0400', tz='America/New_York').replace(hour=9, minute=30, second=0).astimezone('GMT').isoformat()[:-6]+'Z'\n",
    "end   =  pd.Timestamp(f'{end_date} 16:00:00-0400', tz='America/New_York').replace(hour=16, minute=0, second=0).astimezone('GMT').isoformat()[:-6]+'Z'\n",
    "# We set the time frequency at which we want to pull prices\n",
    "timeframe='1Min'\n",
    "\n",
    "# Pull prices from the ALPACA API\n",
    "api = create_alpaca_connection()\n",
    "stocks_ohlcv = api.get_bars(tickers, timeframe, start=start, end=end).df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5edc60-52a5-4bf3-b666-8ee10c226a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_ohlcv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922e4449-4a11-4c92-9b6e-8f823d9918ec",
   "metadata": {},
   "source": [
    "#### Using the `top_class_stocks_buy` filter, create a dictionary called `buy_dict` and assign 'n' to each Ticker (key value) as a placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04ebabc-8d4e-43c0-bab0-c0423972055a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary from y_pred and assign a 'n' to each of them for now as a placeholder.\n",
    "buy_dict = dict.fromkeys(top_class_stocks_buy.index.get_level_values(0), 'n')\n",
    "buy_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e6ce57-91b1-4325-9433-7596f30ee5bf",
   "metadata": {},
   "source": [
    "#### Obtain the total available equity in your account from the Alpaca API and store in a variable called `total_capital`. You will split the capital equally between all selected stocks per the CIO's request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51af4d25-ce95-4085-aeb1-39d943d95772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull the total available equity in our account from the  Alpaca API\n",
    "api = create_alpaca_connection()\n",
    "account = api.get_account()\n",
    "total_capital = float(account.equity)\n",
    "print(f\"Total available capital: {total_capital}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598b84b2-712e-44fa-8b11-cd398a292e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute capital per stock, divide equity in account by number of stocks\n",
    "# Use Alpaca API to pull the equity in the account\n",
    "if len(buy_dict) > 0:\n",
    "    capital_per_stock = float(total_capital)/ len(buy_dict)\n",
    "else:\n",
    "    capital_per_stock = 0\n",
    "print(f'Capital per stock: {capital_per_stock}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf641a93-1d6e-40a9-ac38-790b2d6c5ed5",
   "metadata": {},
   "source": [
    "#### Use a for-loop to iterate through `buy_dict` to determine the number stocks you need to buy for each ticker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5990ee5-09e4-4f01-a48a-fcffd50d7abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use for loop to iterate through dictionary of buys \n",
    "# Determine the number stocks we need to buy for each ticker\n",
    "for ticker in buy_dict:\n",
    "    try:\n",
    "        buy_dict[ticker] = int(capital_per_stock / close_df[ticker].iloc[-1])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(buy_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0949088e-e2f4-4a49-9d38-77577150e28b",
   "metadata": {},
   "source": [
    "#### Cancel all previous orders in the Alpaca API (so you don't buy more than intended) and sell all currently held stocks to close all positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df93674-f704-4f05-9d15-d957e287ddf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cancel all previous orders in the Alpaca API\n",
    "api.cancel_all_orders()\n",
    "\n",
    "# Sell all currently held stocks to close all positions\n",
    "api.close_all_positions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab17547-889b-499d-bd89-b334fe458368",
   "metadata": {},
   "source": [
    "#### Iterate through `buy_dict` and send a buy order for each ticker with their corresponding number of shares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189ff217-8426-4f26-b4a4-504272d54af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the buy_dict object and send a buy order for each ticker with a corresponding number of shares:\n",
    "for stock, qty in buy_dict.items():    \n",
    "    # Submit a market order to buy shares as described in buy_dict\n",
    "    api.submit_order(\n",
    "        symbol=stock,\n",
    "        qty=qty,\n",
    "        side='buy',\n",
    "        type='market',\n",
    "        time_in_force='gtc',\n",
    "    )\n",
    "    print(f'buying {stock} numShares {qty}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5adff4b-f254-4aab-ad20-f8ac10a160a8",
   "metadata": {},
   "source": [
    "### Automate the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88aac7f5-ef14-4959-93da-b818e8d8c4ad",
   "metadata": {},
   "source": [
    "#### Create `trade()` function that incorporates all of the steps above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a281d7d1-9b3b-4023-9e1f-b5a3f1921011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add all of the steps conducted above into the function trade\n",
    "#def trade():\n",
    "## TO DO\n",
    "tickers = top_class_stocks.index\n",
    "# Notice that we remove the start and end variables since we want the latest prices.\n",
    "timeframe='1Min'\n",
    "# Use iloc to get the last 10 mins every time we pull new data\n",
    "prices = api.get_barset(ticker_list, \"minute\").df.iloc[-11:]\n",
    "prices.ffill(inplace=True)   \n",
    "prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba384f2-de5f-4f4f-8798-008be371133f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b517b0-53c8-4ec6-a054-0d2b3c9b768e",
   "metadata": {},
   "source": [
    "#### mport Python's schedule module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1f3550-6671-4f27-a68c-25a5ecddcf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python's schedule module \n",
    "# All imports have been completed at the start of the code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931b424d-a186-4b20-a8f9-14fc1ad704a4",
   "metadata": {},
   "source": [
    "#### Use the \"schedule\" module to automate the algorithm:\n",
    "* Clear the schedule with `.clear()`.\n",
    "* Define a schedule to run the trade function every minute at 5 seconds past the minute mark (e.g. `10:31:05`).\n",
    "* Use the Alpaca API to check whether the market is open.\n",
    "* Use run_pending() function inside schedule to execute the schedule you defined while the market is open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64842359-d587-48fb-8c79-14bafc673a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the schedule\n",
    "schedule.clear()\n",
    "\n",
    "# Define a schedule to run the trade function every minute at 5 seconds past the minute mark (e.g. 10:31:05)\n",
    "trade_schedule = schedule.every().minute.at(\":05\").do(trade)\n",
    "\n",
    "# Use the Alpaca API to check whether the market is open\n",
    "clock = api.get_clock()\n",
    "\n",
    "# Use run_pending() function inside schedule to execute the schedule you defined as long as the market is open\n",
    "while clock.is_open == True:\n",
    "    print(f'The market trading widow for {clock.next_open} is open, executing trade function')\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1)\n",
    "else:\n",
    "    print(f'The market is closed the next open market day will be {clock.next_open}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2376f7-9dc3-4b84-8532-5e196bb24ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Scheduled Jobs\n",
    "schedule.get_jobs()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
